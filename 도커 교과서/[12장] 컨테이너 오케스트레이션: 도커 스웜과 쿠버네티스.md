운영 환경에서도 여전히 애플리케이션은 컨테이너에서 실행되지만, 여러 대의 도커 호스트와 컨테이너를 관리해주는 관리 레이거가 한 층 추가된다. 이 관리 레이어를 오케스트레이션이라고 하며, 주요 오케스트레이션 도구로 도커 스웜(Docker Swarm)과 쿠버네티스(Kubernetes)가 있다.

# 12.1 컨테이너 오케스트레이션 도구란?

도커 컴포즈는 단일 도커 호스트에서 컨테이너를 실행하기 위한 도구였다. 하지만 운영 환경은 단일 호스트로만 구성되지 않는다. 단일 호스트 환경에서는 호스트 한 대만 고장을 일으켜도 전체 애플리케이션이 중단된다. 서비스 운영을 위한 시스템은 고가용성을 요구하는데, 이 때문에 오케스트레이션이 필요해진다. 오케스트레이션 도구란 기본적으로 클러스터를 구성하는 여러 대의 호스트 컴퓨터를 의미한다. 오케스트레이션 도구는 컨테이너를 관리하고, 서비스를 제공하기 위한 작업을 여러 컴퓨터에 분배하며, 네트워크 트래픽 부하를 고르게 분산시키고, 상태가 불량한 컨테이너를 새 컨테이너로 교체하는 일을 담당한다.

여러 대의 호스트에 도커를 설치해 클러스터를 만들고 나면, 이들 컴퓨터를 오케스트레이션 플랫폼에 등록한다. 그 다음부터는 명령행 도구나 웹 UI를 통해 원격에서 클러스터를 관리할 수 있다.

오케스트레이션 도구에는 컨테이너의 잠재력을 한층 더 살릴 수 있는 기능이 있다. 클러스터에는 클러스터에 배포된 애플리케이션에 대한 모든 정보가 담긴 분산 데이터베이스와 어떤 컨테이너를 어떤 호스트에서 실행할지 배정하는 스케줄러, 클러스터를 구성하는 호스트 간에 주기적으로 연락 신호를 주고받는 시스템이 있는데, 이들 시스템은 클러스터의 신뢰성을 확보하는 기본적인 수단이 된다. 클러스터에 애플리케이션을 배포하려면 클러스터에 YAML 파일을 전달하면 된다. 그러면 클러스터가 애플리케이션 구성 정보를 저장하고 그에 맞춰 동원 가능한 서버에서 컨테이너를 생성해 애플리케이션을 실행한다. 애플리케이션이 실행되면 클러스터는 애플리케이션의 실행 상태가 유지되도록 관리한다. 어떤 서버가 고장 나면 컨테이너 일부가 손실되겠지만, 클러스터가 다른 서버에서 대체 컨테이너를 실행한다.

복잡한 컨테이너 관리는 모두 오케스트레이션 도구가 대신해 준다. 우리는 YAML에 원하는 애플리케이션의 상태를 작성하기만 하면 되며, 어떤 서버에서 몇 개의 컨테이너를 실행해야 할지는 신경 쓰지 않아도 된다. 게다가 오케스트레이션 도구는 네트워크 관련 기능, 애플리케이션 설정 기능, 데이터 저장 기능도 제공한다.

# 12.2 도커 스웜으로 클러스터 만들기

도커 스웜은 도커 엔진에 포함돼 있어 별도의 설치가 필요 없다. 도커 엔진을 스웜 모드로 전환해 클러스터를 초기화하면 된다.

```yaml
root@ip-172-31-0-62:/home/ubuntu# docker swarm init
Swarm initialized: current node (0fscz2n0v6f739ucxm35z55vb) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-67k6o2fkavqy17zxdftgjrjjpbm2mosdgf7qm56ybqri7zxg3m-d804dr9ty61izfq60ctbwuaje 172.31.0.62:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructio
```

도커 스웜이 초기화되고 나의 컴퓨터가 클러스터 매니저가 됐다. **클러스터에 속한 컴퓨터는 매니저와 워커라는 두 가지 역할 중 하나를 맡는다.** 출력된 내용 중에는 다른 컴퓨터를 클러스터에 워커로 참여시키기 위해 입력할 명령어도 있다.

**매니저는 클러스터를 관리하는 작업을 직접 수행한다.** 클러스터 데이터베이스도 매니저 노드에 저장되며, 사용자가 YAML 파일을 전달하기 위해 사용하는 API도 매니저 노드에서 동작하고 컨테이너 모니터링과 스케줄링 모두 매니저 노드가 수행한다. 이와 달리 **워커는 매니저의 스케줄링에 따라 컨테이너를 실행하고 그 상태를 주기적으로 매니저에 보고하는 역할**을 하는데, 매니저도 워커의 역할을 수행할 수는 있다.

스웜을 만들고 나면 원하는 만큼 컴퓨터를 스웜에 추가할 수 있다. **스웜에 추가된 컴퓨터를 노드라고 부른다.** 스웜에 노드로 추가하려면, 먼저 해당 컴퓨터가 스웜과 같은 네트워크상에 있어야 하고 스웜에 들어가기 위한 패스워드 역할을 하는 참가 토큰을 매니저로부터 발급받아야 한다. 매니저 노드에 접근이 가능하다면 매니저용 혹은 워커용 참가 토큰을 출력하거나 스웜에 현재 참여중인 노드의 목록을 볼 수 있다.

```yaml
root@ip-172-31-0-62:/home/ubuntu# docker swarm init
Swarm initialized: current node (0fscz2n0v6f739ucxm35z55vb) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-67k6o2fkavqy17zxdftgjrjjpbm2mosdgf7qm56ybqri7zxg3m-d804dr9ty61izfq60ctbwuaje 172.31.0.62:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.
root@ip-172-31-0-62:/home/ubuntu# sudo ufw allow 2377
Rules updated
Rules updated (v6)
```

```yaml
root@ip-172-31-0-63:/home/ubuntu# docker swarm join --token SWMTKN-1-67k6o2fkavqy17zxdftgjrjjpbm2mosdgf7qm56ybqri7zxg3m-d804dr9ty61izfq60ctbwuaje 172.31.0.62:2377
Error response from daemon: Timeout was reached before node joined. The attempt to join the swarm will continue in the background. Use the "docker info" command to see the current swarm status of your node.
root@ip-172-31-0-63:/home/ubuntu# ufw allow 2377
Rules updated
Rules updated (v6)
root@ip-172-31-0-63:/home/ubuntu# docker swarm join --token SWMTKN-1-67k6o2fkavqy17zxdftgjrjjpbm2mosdgf7qm56ybqri7zxg3m-d804dr9ty61izfq60ctbwuaje 172.31.0.62:2377
Error response from daemon: This node is already part of a swarm. Use "docker swarm leave" to leave this swarm and join another one.
root@ip-172-31-0-63:/home/ubuntu# docker swarm leave
Node left the swarm.
root@ip-172-31-0-63:/home/ubuntu# docker swarm join --token SWMTKN-1-67k6o2fkavqy17zxdftgjrjjpbm2mosdgf7qm56ybqri7zxg3m-d804dr9ty61izfq60ctbwuaje 172.31.0.62:2377
This node joined a swarm as a worker.
```

```yaml
root@ip-172-31-0-62:/home/ubuntu# docker swarm join-token worker
To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-67k6o2fkavqy17zxdftgjrjjpbm2mosdgf7qm56ybqri7zxg3m-d804dr9ty61izfq60ctbwuaje 172.31.0.62:2377

root@ip-172-31-0-62:/home/ubuntu# docker swarm join-token manager
To add a manager to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-67k6o2fkavqy17zxdftgjrjjpbm2mosdgf7qm56ybqri7zxg3m-7mq8cw78h30agh22fb3vyarok 172.31.0.62:2377

root@ip-172-31-0-62:/home/ubuntu# docker node ls
ID                            HOSTNAME         STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
0fscz2n0v6f739ucxm35z55vb *   ip-172-31-0-62   Ready     Active         Leader           28.2.2
z6slbc04lubknht7yx4hwcg45     ip-172-31-0-63   Ready     Active                          28.2.2
```

단일 노드 스웜도 노드가 여러 개인 스웜과 같은 방식으로 동작한다. 다른 점이 있다면, 노드가 하나인 만큼 노드가 여러 개인 스웜에 비해 높은 가용성을 가질 수 없으며 컨테이너 수를 원하는 만큼 증가시키는 스케일링이 불가능하다는 점이다.

도커 스웜이 쿠버네티스보다 나은 점 중 하나는 클러스터를 구성하고 관리하는 작업이 단순하다는 것이다. 수십대 정도 규모의 스웜은 각 호스트마다 도커를 설치하고 매니저 노드를 맡을 컴퓨터에서 `docker swarm init` 명령을 한 번 실행한 다음, 나머지 노드에서 `docker swarm join` 명령을 실행하는 정도면 만들 수 있다.

# 12.3 도커 스웜 서비스로 애플리케이션 실행하기

도커 스웜 환경에서는 컨테이너를 직접 실행할 필요가 없다. 서비스를 배포하면 스웜이 대신 컨테이너를 실행해준다. 서비스는 컨테이너를 추상화한 개념이다. 하나의 서비스가 여러 개의 컨테이너로 배포될 수 있다는 점에서 스웜에서 말하는 서비스는 컴포즈에서 말하는 서비스와 의미가 같다.

서비스는 컨테이너와 같은 정보로 정의된다. 사용되는 이미지, 환경 변수와 값, 공개되는 포트와 같은 정보다. 여기에 서비스의 이름이 도커 네트워크 상에서 그대로 도메인으로 사용된다는 점도 컨테이너와 같다. 차이점이라면, 서비스는 여러 개의 레플리카(서비스와 똑같이 설정되지만 스웜상의 여러 노드에 흩어져 배치될 수 있다)를 가질 수 있다는 점이다.

```yaml
root@ip-172-31-0-62:/home/ubuntu# docker service create --name timecheck --replicas 1 diamol/ch12-timecheck:1.0
k5lsrdam4memi2pmkabtxkbb1
overall progress: 1 out of 1 tasks 
1/1: running   [==================================================>] 
verify: Service k5lsrdam4memi2pmkabtxkbb1 converged
root@ip-172-31-0-62:/home/ubuntu# docker service ls
ID             NAME        MODE         REPLICAS   IMAGE                       PORTS
k5lsrdam4mem   timecheck   replicated   1/1        diamol/ch12-timecheck:1.0    
```

서비스는 도커 스웜의 일급 객체지만, 서비스를 다루려면 도커 엔진이 스웜 모드이거나 스웜 매니저에 연결된 상태여야 한다. 실행 결과를 보면 서비스가 생성되고 서비스 목록에서 서비스의 기본적인 정보가 출력된다. 이 서비스는 하나의 레플리카를 실행중임을 알 수 있다.

**서비스를 구성하는 컨테이너를 레플리카(replica)라고 부른다.** 그러나 레플리카는 뭔가 특별한 것이 아니라 평범한 도커 컨테이너다. 레플리카를 실행 중인 노드에 접속하면, 지금까지 컨테이너를 다룰 때와 마찬가지로 `docker container` 명령을 사용할 수 있다. 노드가 하나뿐인 스웜에서는 모든 레플리카가 같은 서버에서 실행되므로 지금 만든 서비스 컨테이너를 바로 다룰 수 있다. 컨테이너 관리를 스웜이 대신 해주니 컨테이너를 직접 다룰 일이 많지는 않겠지만, 그럴 필요가 있다면 직접 다룰 수 있다.

```yaml
root@ip-172-31-0-62:/home/ubuntu# docker service ps timecheck
ID             NAME          IMAGE                       NODE             DESIRED STATE   CURRENT STATE           ERROR     PORTS
pvia0kdhxko1   timecheck.1   diamol/ch12-timecheck:1.0   ip-172-31-0-62   Running         Running 4 minutes ago             
root@ip-172-31-0-62:/home/ubuntu# docker container ls
CONTAINER ID   IMAGE                       COMMAND                  CREATED         STATUS         PORTS     NAMES
6ff91e9938c1   diamol/ch12-timecheck:1.0   "dotnet TimeCheck.dll"   4 minutes ago   Up 4 minutes             timecheck.1.pvia0kdhxko10q14p3704d8df
root@ip-172-31-0-62:/home/ubuntu# docker container rm -f $(docker container ls --last 1 -q)
6ff91e9938c1
root@ip-172-31-0-62:/home/ubuntu# docker service ps timecheck
ID             NAME              IMAGE                       NODE             DESIRED STATE   CURRENT STATE            ERROR                         PORTS
kbgnzyggrlxz   timecheck.1       diamol/ch12-timecheck:1.0   ip-172-31-0-62   Running         Running 34 seconds ago                                 
pvia0kdhxko1    \_ timecheck.1   diamol/ch12-timecheck:1.0   ip-172-31-0-62   Shutdown        Failed 39 seconds ago    "task: non-zero exit (137)" 
```

서비스 레플리카가 현재 실행 중이지만, 스웜이 레플리카의 관리를 맡고 있다. 직접 컨테이너를 삭제해 보면 스웜이 레플리카 수가 부족해졌다고 판단하고 새로운 컨테이너를 실행한다.

조금 전 생성한 서비스의 레플리카인 컨테이너가 하나 있었고, 수동으로 그 컨테이너를 삭제했다. **하지만 스웜상에 서비스가 여전히 존재하므로, 내가 수동으로 컨테이너를 삭제하니 스웜은 컨테이너가 부족하다고 판단하고 대체 컨테이너를 실행했다.** 마지막 레플리카 리스트를 보면 원래 있었던 컨테이너는 실패 상태로 나오지만, 스웜의 입장에서는 컨테이너가 중지된 이유를 알 수 없다.

도커 엔진을 스웜 모드로 전환했다면, 애플리케이션을 서비스로 보고 각각의 컨테이너를 관리하는 것은 스웜에 맡겨야 한다. 모든 컨테이너를 직접 관리하려면 스웜에 참가 중인 노드에 일일이 접속해 해당 노드가 서비스에 포함되는 레플리카를 실행 중인지 확인한 후 컨테이너를 관리해야 하는데, 이것이 사실상 불가능하기 때문이다. 그 대신 도커가 스웜 리소스를 관리할 수 있는 명령을 제공한다. `docker service` 명령을 사용해 레플리카의 로그를 확인하거나 서비스의 구성을 확인할 수 있다.

```yaml
root@ip-172-31-0-62:/home/ubuntu# docker service logs --since 10s timecheck
timecheck.1.kbgnzyggrlxz@ip-172-31-0-62    | App version: 1.0; time check: 13:27.36
timecheck.1.kbgnzyggrlxz@ip-172-31-0-62    | App version: 1.0; time check: 13:27.41
root@ip-172-31-0-62:/home/ubuntu# docker service inspect timecheck -f '{{.Spec.TaskTemplate.ContainerSpec.Image}}'
diamol/ch12-timecheck:1.0@sha256:9d3010a572344c988da8e28444ed345c63662a5c211886e670a8ef3c84689b4e
```

스웜 모드에서는 `docker service` 명령을 사용해 애플리케이션을 관리한다. 이 명령으로 레플리카의 로그 같은 각 레플리카의 정보나 서비스에 대한 전반적인 정보를 확인할 수 있다. 서비스의 레플리카에서 10초 동안 생성된 로그와 서비스 구성 정보가 출력됐다.

서비스 전체의 구성 정보는 클러스터에 저장되어 있으므로, 포맷 파라미터 없이 `service inspect` 명령을 입력하면 확인할 수 있다. 클러스터 데이터베이스에는 상당히 많은 정보가 들어 있는데, 이들 정보는 안전하게 암호화돼 모든 매니저 노드마다 복제본이 위치한다. 도커 컴포즈가 도커 스웜과 가장 크게 다른 점은 애플리케이션 정의를 저장할 공간을 갖지 않는다는 것이다. 애플리케이션 정의는 컴포즈 파일에만 들어 있으므로 도커 컴포즈로 애플리케이션을 관리하려면 컴포즈 파일이 있어야 한다. **스웜 모드에서는 애플리케이션 정의가 클러스터에 저장된다. 그러므로 로컬 컴퓨터에 YAML 파일을 갖고 있지 않아도 원격에서 애플리케이션을 관리할 수 있다.**

지금 실행중인 서비스를 수정해 이를 실제로 확인해보자. 이미지 버전을 새로운 버전으로 변경할 수 있지만 그 외에 다른 서비스 구성 정보는 입력하지 않아도 된다. 클러스터에 배포된 애플리케이션은 이런 방식으로 수정한다. 서비스 정의를 변경하면 스웜이 레플리카를 하나씩 새로운 것으로 교체하며 변경 사항을 적용한다.

```yaml
root@ip-172-31-0-62:/home/ubuntu# docker service update --image diamol/ch12-timecheck:2.0 timecheck
timecheck
overall progress: 1 out of 1 tasks 
1/1: running   [==================================================>] 
verify: Service timecheck converged 
root@ip-172-31-0-62:/home/ubuntu# docker service ps timecheck
ID             NAME              IMAGE                       NODE             DESIRED STATE   CURRENT STATE             ERROR                         PORTS
x1vvzlz3jzd3   timecheck.1       diamol/ch12-timecheck:2.0   ip-172-31-0-62   Running         Running 17 seconds ago                                  
kbgnzyggrlxz    \_ timecheck.1   diamol/ch12-timecheck:1.0   ip-172-31-0-62   Shutdown        Shutdown 20 seconds ago                                 
pvia0kdhxko1    \_ timecheck.1   diamol/ch12-timecheck:1.0   ip-172-31-0-62   Shutdown        Failed 32 minutes ago     "task: non-zero exit (137)"   
root@ip-172-31-0-62:/home/ubuntu# docker service logs --since 20s timecheck
timecheck.1.x1vvzlz3jzd3@ip-172-31-0-62    | App version: 2.0; time check: 13:50.34
timecheck.1.x1vvzlz3jzd3@ip-172-31-0-62    | App version: 2.0; time check: 13:50.39
timecheck.1.x1vvzlz3jzd3@ip-172-31-0-62    | App version: 2.0; time check: 13:50.44
timecheck.1.x1vvzlz3jzd3@ip-172-31-0-62    | App version: 2.0; time check: 13:50.49
```

`service ps` 명령으로 레플리카 목록을 확인하면 두 개의 인스턴스가 있을 것이다. 그 중 오래된 것은 태그가 1.0인 이미지로 실행된 것이고, 새로 실행된 것은 태그가 2.0인 이미지로 실행된 것이다. 각 로그에는 레플리카의 식별자가 달려 있어 어느 레플리카에서 출력한 것인지 알 수 있다. 이 로그는 애플리케이션에서 컨테이너에 남긴 것으로, 스웜이 이를 다시 수집하고 레플리카 식별자를 달아 보여주는 것이다.